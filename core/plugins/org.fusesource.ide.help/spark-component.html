<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Apache Spark Component</title><link rel="stylesheet" type="text/css" href="eclipse_book.css"><meta name="generator" content="DocBook XSL Stylesheets V1.77.1"><link rel="home" href="index.html" title="Red Hat Fuse Tooling"><link rel="up" href="IDU-Components.html" title="Part&nbsp;V.&nbsp;Apache Camel Component Reference"><link rel="prev" href="solr-component.html" title="Solr Component"><link rel="next" href="spark-rest-component.html" title="Spark Rest Component"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spark-component"></a>Apache Spark Component</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="spark-component.html#_supported_architectural_styles">Supported architectural styles</a></span></dt><dt><span class="section"><a href="spark-component.html#_running_spark_in_osgi_servers">Running Spark in OSGi servers</a></span></dt><dt><span class="section"><a href="spark-component.html#_uri_format_181">URI format</a></span></dt><dt><span class="section"><a href="spark-component.html#_dataframe_jobs">DataFrame jobs</a></span></dt><dt><span class="section"><a href="spark-component.html#_hive_jobs">Hive jobs</a></span></dt><dt><span class="section"><a href="spark-component.html#_see_also_120">See Also</a></span></dt></dl></div><p><span class="strong"><strong>Available as of Camel version 2.17</strong></span></p><p>This documentation page covers the <a class="link" href="http://spark.apache.org/" target="_top">Apache
Spark</a> component for the Apache Camel. The main purpose of the Spark
integration with Camel is to provide a bridge between Camel connectors
and Spark tasks. In particular Camel connector provides a way to route
message from various transports, dynamically choose a task to execute,
use incoming message as input data for that task and finally deliver the
results of the execution back to the Camel pipeline.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_supported_architectural_styles"></a>Supported architectural styles</h2></div></div></div><p>Spark component can be used as a driver application deployed into an
application server (or executed as a fat jar).</p><p><span class="inlinemediaobject"><img src="images/camel_spark_driver.png" alt="image"></span></p><p>Spark component can also be submitted as a job directly into the Spark
cluster.</p><p><span class="inlinemediaobject"><img src="images/camel_spark_cluster.png" alt="image"></span></p><p>While Spark component is primary designed to work as a <span class="emphasis"><em>long running
job</em></span>&nbsp;serving as an bridge between Spark cluster and the other endpoints,
you can also use it as a <span class="emphasis"><em>fire-once</em></span> short job. &nbsp;&nbsp;</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_running_spark_in_osgi_servers"></a>Running Spark in OSGi servers</h2></div></div></div><p>Currently the Spark component doesn&#8217;t support execution in the OSGi
container. Spark has been designed to be executed as a fat jar, usually
submitted as a job to a cluster. For those reasons running Spark in an
OSGi server is at least challenging and is not support by Camel as well.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_uri_format_181"></a>URI format</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="spark-component.html#_spark_options">Spark options</a></span></dt><dt><span class="section"><a href="spark-component.html#_path_parameters_1_parameters_161">Path Parameters (1 parameters):</a></span></dt><dt><span class="section"><a href="spark-component.html#_query_parameters_6_parameters_14">Query Parameters (6 parameters):</a></span></dt><dt><span class="section"><a href="spark-component.html#_void_rdd_callbacks">Void RDD callbacks</a></span></dt><dt><span class="section"><a href="spark-component.html#_converting_rdd_callbacks">Converting RDD callbacks</a></span></dt><dt><span class="section"><a href="spark-component.html#_annotated_rdd_callbacks">Annotated RDD callbacks</a></span></dt></dl></div><p>Currently the Spark component supports only producers - it it intended
to invoke a Spark job and return results. You can call RDD, data frame
or Hive SQL job.</p><p><span class="strong"><strong>Spark URI format</strong></span></p><pre class="programlisting">spark:{rdd|dataframe|hive}</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_spark_options"></a>Spark options</h3></div></div></div><p>The Apache Spark component supports 3 options which are listed below.</p><div class="informaltable"><table border="1" width="100%"><colgroup><col width="19%" class="col_1"><col width="51%" class="col_2"><col width="10%" class="col_3"><col width="20%" class="col_4"></colgroup><thead><tr><th align="left" valign="top">Name</th><th align="left" valign="top">Description</th><th align="center" valign="top">Default</th><th align="left" valign="top">Type</th></tr></thead><tbody><tr><td align="left" valign="top"><p><span class="strong"><strong>rdd</strong></span> (producer)</p></td><td align="left" valign="top"><p>RDD to compute against.</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>JavaRDDLike</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>rddCallback</strong></span> (producer)</p></td><td align="left" valign="top"><p>Function performing action against an RDD.</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>RddCallback</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>resolveProperty Placeholders</strong></span> (advanced)</p></td><td align="left" valign="top"><p>Whether the component should resolve property placeholders on itself when starting. Only properties which are of String type can use property placeholders.</p></td><td align="center" valign="top"><p>true</p></td><td align="left" valign="top"><p>boolean</p></td></tr></tbody></table></div><p>The Apache Spark endpoint is configured using URI syntax:</p><pre class="screen">spark:endpointType</pre><p>with the following path and query parameters:</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_path_parameters_1_parameters_161"></a>Path Parameters (1 parameters):</h3></div></div></div><div class="informaltable"><table border="1" width="100%"><colgroup><col width="19%" class="col_1"><col width="51%" class="col_2"><col width="10%" class="col_3"><col width="20%" class="col_4"></colgroup><thead><tr><th align="left" valign="top">Name</th><th align="left" valign="top">Description</th><th align="center" valign="top">Default</th><th align="left" valign="top">Type</th></tr></thead><tbody><tr><td align="left" valign="top"><p><span class="strong"><strong>endpointType</strong></span></p></td><td align="left" valign="top"><p><span class="strong"><strong>Required</strong></span> Type of the endpoint (rdd, dataframe, hive).</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>EndpointType</p></td></tr></tbody></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_query_parameters_6_parameters_14"></a>Query Parameters (6 parameters):</h3></div></div></div><div class="informaltable"><table border="1" width="100%"><colgroup><col width="19%" class="col_1"><col width="51%" class="col_2"><col width="10%" class="col_3"><col width="20%" class="col_4"></colgroup><thead><tr><th align="left" valign="top">Name</th><th align="left" valign="top">Description</th><th align="center" valign="top">Default</th><th align="left" valign="top">Type</th></tr></thead><tbody><tr><td align="left" valign="top"><p><span class="strong"><strong>collect</strong></span> (producer)</p></td><td align="left" valign="top"><p>Indicates if results should be collected or counted.</p></td><td align="center" valign="top"><p>true</p></td><td align="left" valign="top"><p>boolean</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>dataFrame</strong></span> (producer)</p></td><td align="left" valign="top"><p>DataFrame to compute against.</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>DataFrame</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>dataFrameCallback</strong></span> (producer)</p></td><td align="left" valign="top"><p>Function performing action against an DataFrame.</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>DataFrameCallback</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>rdd</strong></span> (producer)</p></td><td align="left" valign="top"><p>RDD to compute against.</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>JavaRDDLike</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>rddCallback</strong></span> (producer)</p></td><td align="left" valign="top"><p>Function performing action against an RDD.</p></td><td align="center" valign="top">&nbsp;</td><td align="left" valign="top"><p>RddCallback</p></td></tr><tr><td align="left" valign="top"><p><span class="strong"><strong>synchronous</strong></span> (advanced)</p></td><td align="left" valign="top"><p>Sets whether synchronous processing should be strictly used, or Camel is allowed to use asynchronous processing (if supported).</p></td><td align="center" valign="top"><p>false</p></td><td align="left" valign="top"><p>boolean</p></td></tr></tbody></table></div><p>&nbsp;
<span class="marked">#</span> RDD jobs&nbsp;</p><p>To invoke an RDD job, use the following URI:</p><p><span class="strong"><strong>Spark RDD producer</strong></span></p><pre class="programlisting">spark:rdd?rdd=#testFileRdd&amp;rddCallback=#transformation</pre><p>&nbsp;Where <code class="literal">rdd</code> option refers to the name of an RDD instance (subclass of
<code class="literal">org.apache.spark.api.java.JavaRDDLike</code>) from a Camel registry, while
<code class="literal">rddCallback</code> refers to the implementation
of&nbsp;<code class="literal">org.apache.camel.component.spark.RddCallback</code> interface (also from a
registry). RDD callback provides a single method used to apply incoming
messages against the given RDD. Results of callback computations are
saved as a body to an exchange.</p><p><span class="strong"><strong>Spark RDD callback</strong></span></p><pre class="programlisting"><strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">interface</strong> RddCallback&lt;T&gt; {
    T onRdd(JavaRDDLike rdd, Object... payloads);
}</pre><p>The following snippet demonstrates how to send message as an input to
the job and return results:</p><p><span class="strong"><strong>Calling spark job</strong></span></p><pre class="programlisting">String pattern = <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"job input"</em></strong>;
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong> linesCount = producerTemplate.requestBody(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"spark:rdd?rdd=#myRdd&amp;rddCallback=#countLinesContaining"</em></strong>, pattern, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong>.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>);</pre><p>The RDD callback for the snippet above registered as Spring bean could
look as follows:</p><p><span class="strong"><strong>Spark RDD callback</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
RddCallback&lt;Long&gt; countLinesContaining() {
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">new</strong> RddCallback&lt;Long&gt;() {
        Long onRdd(JavaRDDLike rdd, Object... payloads) {
            String pattern = (String) payloads[<span xmlns="http://www.w3.org/1999/xhtml" class="hl-number">0</span>];
            <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> rdd.filter({line -&gt; line.contains(pattern)}).count();
        }
    }
}</pre><p>The RDD definition in Spring could looks as follows:</p><p><span class="strong"><strong>Spark RDD definition</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
JavaRDDLike myRdd(JavaSparkContext sparkContext) {
  <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> sparkContext.textFile(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"testrdd.txt"</em></strong>);
}</pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_void_rdd_callbacks"></a>Void RDD callbacks</h3></div></div></div><p>If your RDD callback doesn&#8217;t return any value back to a Camel pipeline,
you can either return <code class="literal">null</code> value or use&nbsp;<code class="literal">VoidRddCallback</code> base class:</p><p><span class="strong"><strong>Spark RDD definition</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
RddCallback&lt;Void&gt; rddCallback() {
  <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">new</strong> VoidRddCallback() {
        <em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Override</span></em>
        <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">void</strong> doOnRdd(JavaRDDLike rdd, Object... payloads) {
            rdd.saveAsTextFile(output.getAbsolutePath());
        }
    };
}</pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_converting_rdd_callbacks"></a>Converting RDD callbacks</h3></div></div></div><p>If you know what type of the input data will be sent to the RDD
callback, you can use&nbsp;<code class="literal">ConvertingRddCallback</code> and let Camel to
automatically convert incoming messages before inserting those into the
callback:</p><p><span class="strong"><strong>Spark RDD definition</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
RddCallback&lt;Long&gt; rddCallback(CamelContext context) {
  <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">new</strong> ConvertingRddCallback&lt;Long&gt;(context, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong>.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong>.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>) {
            <em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Override</span></em>
            <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> Long doOnRdd(JavaRDDLike rdd, Object... payloads) {
                <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> rdd.count() * (<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong>) payloads[<span xmlns="http://www.w3.org/1999/xhtml" class="hl-number">0</span>] * (<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong>) payloads[<span xmlns="http://www.w3.org/1999/xhtml" class="hl-number">1</span>];
            }
        };
    };
}</pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_annotated_rdd_callbacks"></a>Annotated RDD callbacks</h3></div></div></div><p>Probably the easiest way to work with the RDD callbacks is to provide
class with method marked with&nbsp;<code class="literal">@RddCallback</code> annotation:</p><p><span class="strong"><strong>Annotated RDD callback definition</strong></span></p><pre class="programlisting"><strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">import</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">static</strong> org.apache.camel.component.spark.annotations.AnnotatedRddCallback.annotatedRddCallback;
&nbsp;
<em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
RddCallback&lt;Long&gt; rddCallback() {
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> annotatedRddCallback(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">new</strong> MyTransformation());
}
&nbsp;
...
&nbsp;
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">import</strong> org.apache.camel.component.spark.annotation.RddCallback;
&nbsp;
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong> MyTransformation {
&nbsp;
    <em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@RddCallback</span></em>
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong> countLines(JavaRDD&lt;String&gt; textFile, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong> first, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong> second) {
        <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> textFile.count() * first * second;
    }
&nbsp;
}</pre><p>If you will pass CamelContext to the annotated RDD callback factory
method, the created callback will be able to convert incoming payloads
to match the parameters of the annotated method:</p><p><span class="strong"><strong>Body conversions for annotated RDD callbacks</strong></span></p><pre class="programlisting"><strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">import</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">static</strong> org.apache.camel.component.spark.annotations.AnnotatedRddCallback.annotatedRddCallback;
&nbsp;
<em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
RddCallback&lt;Long&gt; rddCallback(CamelContext camelContext) {
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> annotatedRddCallback(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">new</strong> MyTransformation(), camelContext);
}
&nbsp;
...

&nbsp;
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">import</strong> org.apache.camel.component.spark.annotation.RddCallback;
&nbsp;
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong> MyTransformation {
&nbsp;
    <em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@RddCallback</span></em>
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong> countLines(JavaRDD&lt;String&gt; textFile, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong> first, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">int</strong> second) {
        <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> textFile.count() * first * second;
    }
&nbsp;
}
&nbsp;
...
&nbsp;
<em xmlns="http://www.w3.org/1999/xhtml" class="hl-comment" style="color: silver">// Convert String "10" to integer</em>
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong> result = producerTemplate.requestBody(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"spark:rdd?rdd=#rdd&amp;rddCallback=#rddCallback"</em></strong> Arrays.asList(<span xmlns="http://www.w3.org/1999/xhtml" class="hl-number">10</span>, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"10"</em></strong>), <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong>.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>);</pre><p>&nbsp;</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_dataframe_jobs"></a>DataFrame jobs</h2></div></div></div><p>Instead of working with RDDs Spark component can work with DataFrames as
well.&nbsp;</p><p>To invoke an DataFrame job, use the following URI:</p><p><span class="strong"><strong>Spark RDD producer</strong></span></p><pre class="programlisting">spark:dataframe?dataFrame=#testDataFrame&amp;dataFrameCallback=#transformation</pre><p>&nbsp;Where&nbsp;<code class="literal">dataFrame</code>&nbsp;option refers to the name of an DataFrame instance
(<code class="literal">instance of of&nbsp;org.apache.spark.sql.DataFrame</code>) from a Camel registry,
while&nbsp;<code class="literal">dataFrameCallback</code>&nbsp;refers to the implementation
of&nbsp;<code class="literal">org.apache.camel.component.spark.DataFrameCallback</code>&nbsp;interface (also
from a registry). DataFrame callback provides a single method used to
apply incoming messages against the given DataFrame. Results of callback
computations are saved as a body to an exchange.</p><p><span class="strong"><strong>Spark RDD callback</strong></span></p><pre class="programlisting"><strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">interface</strong> DataFrameCallback&lt;T&gt; {
    T onDataFrame(DataFrame dataFrame, Object... payloads);
}</pre><p>The following snippet demonstrates how to send message as an input to a
job and return results:</p><p><span class="strong"><strong>Calling spark job</strong></span></p><pre class="programlisting">String model = <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"Micra"</em></strong>;
<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong> linesCount = producerTemplate.requestBody(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"spark:dataFrame?dataFrame=#cars&amp;dataFrameCallback=#findCarWithModel"</em></strong>, model, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong>.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>);</pre><p>The DataFrame callback for the snippet above registered as Spring bean
could look as follows:</p><p><span class="strong"><strong>Spark RDD callback</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
RddCallback&lt;Long&gt; findCarWithModel() {
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">new</strong> DataFrameCallback&lt;Long&gt;() {
        <em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Override</span></em>
        <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">public</strong> Long onDataFrame(DataFrame dataFrame, Object... payloads) {
            String model = (String) payloads[<span xmlns="http://www.w3.org/1999/xhtml" class="hl-number">0</span>];
            <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> dataFrame.where(dataFrame.col(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"model"</em></strong>).eqNullSafe(model)).count();
        }
    };
}</pre><p>The DataFrame definition in Spring could looks as follows:</p><p><span class="strong"><strong>Spark RDD definition</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
DataFrame cars(HiveContext hiveContext) {
    DataFrame jsonCars = hiveContext.read().json(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"/var/data/cars.json"</em></strong>);
    jsonCars.registerTempTable(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"cars"</em></strong>);
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> jsonCars;
}</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_hive_jobs"></a>Hive jobs</h2></div></div></div><p>&nbsp;Instead of working with RDDs or DataFrame Spark component can also
receive Hive SQL queries as payloads.&nbsp;To send Hive query to Spark
component, use the following URI:</p><p><span class="strong"><strong>Spark RDD producer</strong></span></p><pre class="programlisting">spark:hive</pre><p>The following snippet demonstrates how to send message as an input to a
job and return results:</p><p><span class="strong"><strong>Calling spark job</strong></span></p><pre class="programlisting"><strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">long</strong> carsCount = template.requestBody(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"spark:hive?collect=false"</em></strong>, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"SELECT * FROM cars"</em></strong>, Long.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>);
List&lt;Row&gt; cars = template.requestBody(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"spark:hive"</em></strong>, <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"SELECT * FROM cars"</em></strong>, List.<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">class</strong>);</pre><p>The table we want to execute query against should be registered in a
HiveContext before we query it. For example in Spring such registration
could look as follows:</p><p><span class="strong"><strong>Spark RDD definition</strong></span></p><pre class="programlisting"><em xmlns="http://www.w3.org/1999/xhtml"><span class="hl-annotation" style="color: gray">@Bean</span></em>
DataFrame cars(HiveContext hiveContext) {
    DataFrame jsonCars = hiveContext.read().json(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"/var/data/cars.json"</em></strong>);
    jsonCars.registerTempTable(<strong xmlns="http://www.w3.org/1999/xhtml" class="hl-string"><em style="color:red">"cars"</em></strong>);
    <strong xmlns="http://www.w3.org/1999/xhtml" class="hl-keyword">return</strong> jsonCars;
}</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_see_also_120"></a>See Also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Configuring Camel</li><li class="listitem">Component</li><li class="listitem">Endpoint</li><li class="listitem">Getting Started</li></ul></div></div></div></body></html>